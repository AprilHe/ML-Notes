{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = open(\"../data/alice.txt\", \"r\") \n",
    "s = sample.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = []  \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:43:15: collecting all words and their counts\n",
      "INFO - 16:43:15: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:43:15: collected 3532 word types from a corpus of 38098 raw words and 1102 sentences\n",
      "INFO - 16:43:15: Loading a fresh vocabulary\n",
      "INFO - 16:43:15: effective_min_count=1 retains 3532 unique words (100% of original 3532, drops 0)\n",
      "INFO - 16:43:15: effective_min_count=1 leaves 38098 word corpus (100% of original 38098, drops 0)\n",
      "INFO - 16:43:16: deleting the raw counts dictionary of 3532 items\n",
      "INFO - 16:43:16: sample=0.001 downsamples 51 most-common words\n",
      "INFO - 16:43:16: downsampling leaves estimated 24840 word corpus (65.2% of prior 38098)\n",
      "INFO - 16:43:16: estimated required memory for 3532 words and 100 dimensions: 4591600 bytes\n",
      "INFO - 16:43:16: resetting layer weights\n",
      "INFO - 16:43:16: training model with 3 workers on 3532 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:16: EPOCH - 1 : training on 38098 raw words (24872 effective words) took 0.1s, 410126 effective words/s\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:17: EPOCH - 2 : training on 38098 raw words (24908 effective words) took 0.1s, 467937 effective words/s\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:17: EPOCH - 3 : training on 38098 raw words (24917 effective words) took 0.1s, 445243 effective words/s\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:17: EPOCH - 4 : training on 38098 raw words (24863 effective words) took 0.1s, 486121 effective words/s\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:17: EPOCH - 5 : training on 38098 raw words (24850 effective words) took 0.1s, 355781 effective words/s\n",
      "INFO - 16:43:17: training on a 190490 raw words (124410 effective words) took 0.4s, 341530 effective words/s\n",
      "WARNING - 16:43:17: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9994733\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9893601\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:43:42: collecting all words and their counts\n",
      "INFO - 16:43:42: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:43:42: collected 3532 word types from a corpus of 38098 raw words and 1102 sentences\n",
      "INFO - 16:43:42: Loading a fresh vocabulary\n",
      "INFO - 16:43:42: effective_min_count=1 retains 3532 unique words (100% of original 3532, drops 0)\n",
      "INFO - 16:43:42: effective_min_count=1 leaves 38098 word corpus (100% of original 38098, drops 0)\n",
      "INFO - 16:43:42: deleting the raw counts dictionary of 3532 items\n",
      "INFO - 16:43:42: sample=0.001 downsamples 51 most-common words\n",
      "INFO - 16:43:42: downsampling leaves estimated 24840 word corpus (65.2% of prior 38098)\n",
      "INFO - 16:43:42: estimated required memory for 3532 words and 100 dimensions: 4591600 bytes\n",
      "INFO - 16:43:42: resetting layer weights\n",
      "INFO - 16:43:43: training model with 3 workers on 3532 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:43: EPOCH - 1 : training on 38098 raw words (24806 effective words) took 0.1s, 259055 effective words/s\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:43: EPOCH - 2 : training on 38098 raw words (24876 effective words) took 0.1s, 254906 effective words/s\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:43: EPOCH - 3 : training on 38098 raw words (24782 effective words) took 0.1s, 251994 effective words/s\n",
      "INFO - 16:43:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:44: EPOCH - 4 : training on 38098 raw words (24858 effective words) took 0.1s, 272734 effective words/s\n",
      "INFO - 16:43:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:44: EPOCH - 5 : training on 38098 raw words (24787 effective words) took 0.1s, 248167 effective words/s\n",
      "INFO - 16:43:44: training on a 190490 raw words (124109 effective words) took 0.5s, 235950 effective words/s\n",
      "WARNING - 16:43:44: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.8719112\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.842918\n"
     ]
    }
   ],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1) \n",
    "  \n",
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - Skip Gram : \", \n",
    "      model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - Skip Gram : \", \n",
    "      model2.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "- [Word Embedding using Word2Vec](https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/)\n",
    "- [Gensim Word2Vec Tutorial](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial/data?select=simpsons_dataset.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
