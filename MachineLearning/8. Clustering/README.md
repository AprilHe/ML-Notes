
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

-----

## Contents
- [1. Clustering](#1-Clustering)
- [2. k-means](#2-k-means)
  - [2.1 Cost function](#21-Cost-function)
  - [2.2 Choosing initial centroids](#22-Choosing-initial-centroids)
- [3. Gaussian Mixture Model](#3-Gaussian-Mixture-Model)
- [4. Implementation](#4-Implementation)

## 1. Clustering

Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them. Clustering is an unsupervised learning method. Commonly used method for clustering including **K-MEANS, Gaussian Mixture Model(GMM), Self-Organizing Map(SOM)**

Types of Clustering:
- Centroid-based Clustering: it organizes the data into non-hierarchical clusters and it is efficient but sensitive to initial conditions and outliers. k-means is the most widely-used centroid-based clustering algorithm
- Density-based Clustering: it connects areas of high example density into clusters.These algorithms have difficulty with data of varying densities and high dimensions. Further, by design, these algorithms do not assign outliers to clusters.
- Distribution-based Clustering: it assumes data is composed of distributions, such as Gaussian distributions.
- Hierarchical Clustering: it creates a tree of clusters. which is well suited to hierarchical data, such as taxonomies.

Steps of Clustering
- Assessing clustering tendency (i.e., the clusterability of the data)
- Defining the optimal number of clusters
- Computing partitioning cluster analyses (e.g.: k-means, pam) or hierarchical clustering
- Validating clustering analyses


## 2. K-means

K-means algorithm partition n observations into k clusters where each observation belongs to the cluster with the nearest mean serving as a prototype of the cluster.
- Choose an appropriate value of k as the number of clusters we want
- Randomly pick k number of points as centroids
- Assign each point to the closest cluster centroid
- Re-compute cluster centroids
- Repeat steps 3 and 4 until no improvements are possible to reach global optima.

```
Repeat {
    for i = 1 to m
    c(i) := index (form 1 to K) of cluster centroid closest to x(i)
    for k = 1 to K
    μk := average (mean) of points assigned to cluster k
}
```


### 2.1 Cost function
Using centroid as a prototype for cluster, we are minimizing a cost function "sum of squared error" -

![equation](https://latex.codecogs.com/gif.latex?Cost%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BK%7D%5Csum_%7Bj%5Cin%20C_%7Bi%7D%7Ddist%28c_%7Bi%7D%2C%20x%29%5E2)

Sum of squared error is the sum of square of distances of all points from their respective cluster prototypes

### 2.2 Choosing initial centroids
The k-means algorithm does not guarantee a global optimum and might converge on a local optimum. Therefore choosing initial centroids is a key step in the algorithm.
- Randomly initialize the centroids generally leads to poor results
- Perform multiple runs with different initial centroids and choose the resulting clusters with least sum of squared error. Often this approach does not work either
- Choose initial centroids is to first choose a single point which is the centroid of all points. Then choose a point which is farthest from that point and use it as another centroid. Then choose a point which is farthest from both the previous centroids and so on choose K initial centroids. (Elbow Method)

**Suggestions**
- To reduce computations and to be robust to outliers, we use a sample of points instead of the entire dataset
- Reduce number of K as we need calculate distance between every point and centroid
- Reduce feature dimension, like PCA
- More likely to get local optimal instead of global optimal




## 3. Gaussian Mixture Model

 Instead of treating the data as a bunch of points as K-means, assume that they are all generated by sampling a continuous function. This function is called a generative model. Gaussian Mixture Models are probabilistic models and use the soft clustering approach for distributing the points in different clusters with EM Algorithm.


Suppose we need to assign k number of clusters. There will be  k Gaussian distributions, with the mean and covariance values to be μ1, μ2, .. μk and Σ1, Σ2, .. Σk . Additionally, there is another parameter for each distribution that defines the weight $\pi_i$. The function is

![equation](https://latex.codecogs.com/gif.latex?p%28x%29%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BK%7D%5Cpi_%7Bi%7DN%28x%7Cu_%7Bi%7D%2C%20%5Csum_%7Bi%7D%29)


To find optimal value of mean, covariance and weight, maximal likelihood function is not working as it will give a complicated non-convex function, which is hard to take differentiate. So we apply EM Algorithm in this situation.

The Expectation-Maximization (EM) algorithm is an iterative way to find maximum-likelihood estimates for model parameters when the data is incomplete or has some missing data points or has some hidden variables. EM chooses some random values for the missing data points and estimates a new set of data. These new values are then recursively used to estimate a better first data, by filling up missing points, until the values get fixed.

- Use some method (such as k-means clustering) to assign each observation to a cluster. The assignment does not have to be precise because it will be refined.
- The M Step: Assume that the current assignment to clusters is correct. Compute the maximum likelihood estimates of the within-cluster count, mean, and covariance. From the counts, estimate the mixing probabilities.
- The E Step: Assume the within-cluster statistics are correct. Evaluate the likelihood that each observation belongs to each cluster. Use the likelihoods to update the assignment to clusters.
- Evaluate the mixture log likelihood, which is an overall measure of the goodness of fit. If the log likelihood has barely changed from the previous iteration, assume that the EM algorithm has converged. Otherwise, go to the M step and repeat until convergence.


## 4. Implementation

[GMM]()

[K-Means]()
